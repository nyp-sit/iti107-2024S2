{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024s2/blob/main/session-7/blip_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfYE8mmDUIGg"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install matplotlib transformers datasets accelerate sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH4qEaw8uyvi"
   },
   "source": [
    "# Multimodal Large Language Model\n",
    "\n",
    "BLIP-2, LLaVA, and other Visual LLMs project the visual features from images to language embeddings, using CLIP-like visual encoders.The language embeddings can then be used as input for an LLM that they can be used as the input for an LLM.  This capability enabled interesting use cases such as image captioning and visual Q&A.\n",
    "\n",
    "In this exercise, we will learn how to use the BLIP-2 to perform image captioning and visual Q&A tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due to a bug in transformer version 4.46.3, the BLIP2 model is not working for image captioning case, we will need to update the transformer the main branch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# make sure our transformers version is the latest \n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsLAEG2a5-5r"
   },
   "source": [
    "## BLIP-2 model\n",
    "\n",
    "Let's just load the BLIP model, and the associated processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f6c70d037ab3446f86e75029d5ea9623",
      "72f766cb2c5e4b5b9774350402ae0c91",
      "3003b4a5506d40a7969e81c7fdd1539d",
      "4e4d8436221d4ff4a6b2ac3fe9fa2af0",
      "763593a61acd4af09a6d9d7c0de4db03",
      "a036a5b1eccd4e308b552ab9d443128d",
      "386ef435a1774d4a98bb799cf5e76c6e",
      "0b13d9b5266e4eb2bbf65bbb4ffdd977",
      "44744b11c135462e94bfccbec0f64e71",
      "c4ba9da4ff534aafab41b437f7aaa3ba",
      "17071f98189d4dd49d9af891caf2e080"
     ]
    },
    "id": "jnNrafWu6BSJ",
    "outputId": "84eccc26-c132-4993-f4e2-f70bed10ff24"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load processor and main model\n",
    "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Send the model to GPU to speed up inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI_UWlR8c_Ey"
   },
   "source": [
    "### Preprocessing Images\n",
    "\n",
    "Let us first look at what kind of image processing is done by the Processor. We will use the following as test image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "W6YeV_TEQFAA",
    "outputId": "dd0628a9-5e67-4585-a1c5-96a49161f553"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Load image of a supercar\n",
    "image_url = 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/assets/merlion.png' \n",
    "image = Image.open(urlopen(image_url)).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjK_yyL4-rMA"
   },
   "source": [
    "Let's see what the BLIP-2 processor does to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flZeiDFmQIIg"
   },
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB0fciws_leh"
   },
   "source": [
    "**Questions**\n",
    "\n",
    "- What is the shape of the tensor?\n",
    "- What can you tell from the shape?\n",
    "- What are the max and min values of the pixel values?\n",
    "\n",
    "<details><summary>Click here for answer</summary>\n",
    "\n",
    "The processor will resize the image into 224 × 224-sized image. So if we have a very wide photo, it will be processed into square photo.\n",
    "The image is RGB (axis=1 is 3_\n",
    "Also the pixel values seem to have been normalized, maximum value is 2.0742 and the minimum value is -1.7773\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-59Rt8DDb0j"
   },
   "source": [
    "To display the pixel values as PIL image, we need to do some transformation first.\n",
    "\n",
    "1. As PIL image is represented as (width, height, channels), but torch tensor is presented as (channels, height, width), we need swap the axis around of the original tensor.  We can use np.einsum() to accompanish this easily in 2 steps:\n",
    " -swap position of channel axis to become 3rd axis (ijk->kji)\n",
    " -swap position of width and height (ijk->jik)\n",
    "2. we also need to scale the pixel values to values between 0 and 255, using minmaxscaler. Since minmax scaler works with 2D data, we need to reshape it 2D, by flattening axis=0 (Height) and axis=1 (Width).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "0lHJ7LiKUhWf",
    "outputId": "c666076f-913c-4b75-eaaf-12546c87830b"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy and go from (1, C, H, W) to (W, H, C) in shape\n",
    "image_inputs = inputs[\"pixel_values\"][0].detach().cpu().numpy()\n",
    "image_inputs = np.einsum('ijk->kji', image_inputs)\n",
    "image_inputs = np.einsum('ijk->jik', image_inputs)\n",
    "\n",
    "# Scale image inputs to 0-255 to represent RGB values\n",
    "scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "\n",
    "image_inputs = scaler.fit_transform(image_inputs.reshape(-1, image_inputs.shape[-1])).reshape(image_inputs.shape)\n",
    "image_inputs = np.array(image_inputs, dtype=np.uint8)\n",
    "\n",
    "# Convert numpy array to Image\n",
    "Image.fromarray(image_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoBrN56dBTF"
   },
   "source": [
    "### Preprocessing Text\n",
    "\n",
    "Let’s continue this exploration of the processor with text instead. First, we\n",
    "can access the tokenizer used to tokenize the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHJkauW9dFhB",
    "outputId": "260f9a43-c762-49cd-9f5f-0120cbd7e336"
   },
   "outputs": [],
   "source": [
    "blip_processor.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8fxaf0EGrwc"
   },
   "source": [
    "To explore how GPT2Tokenizer works, we can try it out with a small\n",
    "sentence. We start by converting the sentence to token IDs before converting\n",
    "them back to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21Zh5Rx8QLAy",
    "outputId": "916ccb0b-0b55-4d04-f500-9f261b76eb99"
   },
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "text = \"Her vocalization was remarkably melodic\"\n",
    "tokens = blip_processor.tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = tokens['input_ids'][0]\n",
    "\n",
    "tokens = blip_processor.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojc0bQ_7HRL0"
   },
   "source": [
    "When we inspect the tokens, you might notice a strange symbol at the\n",
    "beginning of some tokens, namely, the Ġ symbol. This is actually supposed\n",
    "to be a space. However, an internal function takes characters in certain code\n",
    "points and moves them up by 256 to make them printable. As a result, the\n",
    "space (code point 32) becomes Ġ (code point 288)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFnL_Yt3elPX",
    "outputId": "468be3fb-a0ba-4c90-f799-b18092e44ab3"
   },
   "outputs": [],
   "source": [
    "# Replace the space token with an underscore\n",
    "tokens = [token.replace(\"Ġ\", \"_\") for token in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDx5elnHeHnT"
   },
   "source": [
    "### Use Case 1: Image Captioning\n",
    "\n",
    "The most straightforward usage of a model like BLIP-2 is to create captions of images that you have in your data. An image is converted to pixel values that the model can read. These pixel values are passed to BLIP-2 to be converted into soft visual prompts that the LLM can use to decide on a proper caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "gFJ9-l8c8u3i",
    "outputId": "f3bb3c8b-e768-4927-dce7-0250ce2e9a2a"
   },
   "outputs": [],
   "source": [
    "# Load an test image\n",
    "image = Image.open(urlopen(image_url)).convert(\"RGB\")\n",
    "\n",
    "# Convert an image into inputs and preprocess it\n",
    "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "YHIoTZa6eEaR",
    "outputId": "4fc6e413-9757-402c-ed2f-a5a09f11ecf8"
   },
   "outputs": [],
   "source": [
    "# Generate image ids to be passed to the decoder (LLM)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "# Generate text from the image ids\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjHfQxWGkVYF"
   },
   "source": [
    "### Use Case 2: Visual Question Answering\n",
    "\n",
    "In the previous example, we showed going from one modality, vision (image), to another, text (caption). Instead of following this linear structure, we can try to present both modalities simultaneously by performing what is called visual question answering. In this particular use case, we give the model an image along with a question about that specific image for it to answer. The model needs to process both the image as well as the question at once.\n",
    "To demonstrate, let’s start with the picture and ask BLIP-2 to describe the image. To do so, we first need to preprocess the image as we did a few times before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "269NtauNFIze"
   },
   "outputs": [],
   "source": [
    "# Load a test image\n",
    "image = Image.open(urlopen(image_url)).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform our visual question answering we need to give BLIP-2 more than just the image, namely the prompt. Without it, the model would generate a caption as it did before. We will ask the model to describe the image we just processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NsMF_hMCIIj"
   },
   "outputs": [],
   "source": [
    "# Visual Question Answering\n",
    "prompt = \"Question: Write down what you see in this picture. Answer:\"\n",
    "\n",
    "# Process both the image and the prompt\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It correctly describes the image. However, this is a rather simple example since our question is essentially asking the model to create a caption. Instead, we can ask follow-up questions in a chat-based manner. To do so, we can give the model our previous conversation, including its answer to our question. We then ask it a follow-up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtpMIgUaScBD"
   },
   "outputs": [],
   "source": [
    "# Chat-like prompting\n",
    "prompt = \"Question: Write down what you see in this picture. Answer: Merlion. Question: Where is the merlion sitting? Answer:\"\n",
    "\n",
    "# Generate output\n",
    "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text = generated_text[0].strip()\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b13d9b5266e4eb2bbf65bbb4ffdd977": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17071f98189d4dd49d9af891caf2e080": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3003b4a5506d40a7969e81c7fdd1539d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b13d9b5266e4eb2bbf65bbb4ffdd977",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_44744b11c135462e94bfccbec0f64e71",
      "value": 2
     }
    },
    "386ef435a1774d4a98bb799cf5e76c6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44744b11c135462e94bfccbec0f64e71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e4d8436221d4ff4a6b2ac3fe9fa2af0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4ba9da4ff534aafab41b437f7aaa3ba",
      "placeholder": "​",
      "style": "IPY_MODEL_17071f98189d4dd49d9af891caf2e080",
      "value": " 2/2 [01:06&lt;00:00, 31.60s/it]"
     }
    },
    "72f766cb2c5e4b5b9774350402ae0c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a036a5b1eccd4e308b552ab9d443128d",
      "placeholder": "​",
      "style": "IPY_MODEL_386ef435a1774d4a98bb799cf5e76c6e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "763593a61acd4af09a6d9d7c0de4db03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a036a5b1eccd4e308b552ab9d443128d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4ba9da4ff534aafab41b437f7aaa3ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6c70d037ab3446f86e75029d5ea9623": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72f766cb2c5e4b5b9774350402ae0c91",
       "IPY_MODEL_3003b4a5506d40a7969e81c7fdd1539d",
       "IPY_MODEL_4e4d8436221d4ff4a6b2ac3fe9fa2af0"
      ],
      "layout": "IPY_MODEL_763593a61acd4af09a6d9d7c0de4db03"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
