{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/iti107-2024s2/blob/main/session-7/clip-zero-shot-image-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot Image Classificaation using CLIP\n",
        "\n",
        "CLIP is a multi-modal embedding model that is trained to learn the joint embedding of image-text pair. As such, CLIP can be used to compare how similar an image is with a text and vice versa.\n",
        "\n",
        "In this notebook, we will see how we can apply CLIP to do zero-shot image classification."
      ],
      "metadata": {
        "id": "mAzIXP-zyNn-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kHb1BJgtDli"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "VDYf7cPAzPjf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4BlPJC8uqQ3"
      },
      "source": [
        "We will use the `frgfm/imagenette` dataset via Hugging Face Datasets. This is a smaller subset of 10 easily classified classes from Imagenet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT1eZ3z9szcA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imagenette = load_dataset(\n",
        "    'frgfm/imagenette',\n",
        "    '320px',\n",
        "    split='validation'\n",
        ")\n",
        "# show dataset info\n",
        "imagenette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zywA08EFtLhG"
      },
      "outputs": [],
      "source": [
        "set(imagenette['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyGWPQahu04z"
      },
      "source": [
        "The dataset contains 10 labels, all stored as integer values. To perform classification with CLIP we need the text content of these labels. Most Hugging Face datasets include the mapping to text labels inside the the dataset info:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-nreFjVu132"
      },
      "outputs": [],
      "source": [
        "labels = imagenette.info.features['label'].names\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHwGWijyu-_L"
      },
      "source": [
        "We format the one-word classes into sentences because we expect CLIP model to have seen more sentence-like text during pretraining, than a single 'word'. For ImageNet it was reported that a 1.3 percentage point improvement in accuracy was achieved using the same prompt template of \"a photo of a {label}\" [1].\n",
        "\n",
        "Prompt templates don’t always improve performance and they should be tested for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtTHwHNUu9BZ"
      },
      "outputs": [],
      "source": [
        "# generate sentences\n",
        "clip_labels = [f\"a photo of a {label}\" for label in labels]\n",
        "clip_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G27ry0MpvKxV"
      },
      "source": [
        "Before we can compare labels and photos, we need to initialize CLIP. We will use the CLIP implementation found via Hugging Face transformers.\n",
        "\n",
        "CLIP processor wraps a CLIP image processor and a CLIP tokenizer into a single processor. CLIP image processor will do the image preprocessing, such as rescaling, normalizing, cropping etc, while CLIP tokenizer is used to tokenize the text, using Byte Pair Encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhexINZYvLl7"
      },
      "outputs": [],
      "source": [
        "# initialization\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(model_id)\n",
        "model = CLIPModel.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1dAswhjvOfo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# if you have CUDA set it to the active device like this\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGgCw07AvbaL"
      },
      "source": [
        "Text transformers cannot read text directly. Instead, they need a set of integer values known as token IDs (or input_ids), where each unique integer represents a word or sub-word (known as a token).\n",
        "\n",
        "We create these token IDs alongside another tensor called the attention mask (used by the transformer’s attention mechanism) using the processor we just initialized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_labels"
      ],
      "metadata": {
        "id": "8NxZ2QQllBjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gx6n34FvUR7"
      },
      "outputs": [],
      "source": [
        "# create label tokens\n",
        "label_tokens = processor(\n",
        "    text=clip_labels,\n",
        "    padding=True,\n",
        "    images=None,\n",
        "    return_tensors='pt'\n",
        ").to(device)\n",
        "\n",
        "print(label_tokens['input_ids'][0])\n",
        "print(processor.tokenizer.decode(label_tokens['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcGCzd0FvlC3"
      },
      "outputs": [],
      "source": [
        "label_tokens['attention_mask'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmesndjCvw8T"
      },
      "source": [
        "Using these transformer-readable tensors, we create the label text embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFphhdswvqYC"
      },
      "outputs": [],
      "source": [
        "# encode tokens to sentence embeddings\n",
        "label_emb = model.get_text_features(**label_tokens)\n",
        "# detach from pytorch gradient computation\n",
        "label_emb = label_emb.detach().cpu().numpy()\n",
        "label_emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Syvuk1Wv7IJ"
      },
      "source": [
        "The vectors that CLIP outputs are not normalized, meaning dot product similarity will give inaccurate results unless the vectors are normalized beforehand. We do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YQptXswwPYe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# normalization\n",
        "label_emb = label_emb / np.linalg.norm(label_emb, axis=0)\n",
        "label_emb.min(), label_emb.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30jYfTAowYHI"
      },
      "source": [
        "All we have left is to work through the same process with the images from our dataset. We will test this with a single image first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPzmoWvawBE1"
      },
      "outputs": [],
      "source": [
        "imagenette[0]['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLRI90rKwdsn"
      },
      "outputs": [],
      "source": [
        "image = processor(\n",
        "    text=None,\n",
        "    images=imagenette[0]['image'],\n",
        "    return_tensors='pt'\n",
        ")['pixel_values'].to(device)\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7iZP5V8wkSx"
      },
      "source": [
        "After processing the image, we return a single (1) three-color channel (3) image width of 224 pixels and a height of 224 pixels. We must process incoming images to normalize and resize them to fit the input size requirements of the ViT model.\n",
        "\n",
        "We can create the image embedding with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqWCTvHJwlf4"
      },
      "outputs": [],
      "source": [
        "img_emb = model.get_image_features(image)\n",
        "img_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGb73bxOwuhO"
      },
      "outputs": [],
      "source": [
        "img_emb = img_emb.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOkba4uNwzfK"
      },
      "source": [
        "From here, all we need to do is calculate the dot product similarity between our image embedding and the ten label text embeddings. The highest score is our predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXsg_5oqwvu2"
      },
      "outputs": [],
      "source": [
        "scores = np.dot(img_emb, label_emb.T)\n",
        "scores.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjm4k741w5GR"
      },
      "outputs": [],
      "source": [
        "# get index of highest score\n",
        "pred = np.argmax(scores)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9Y8wWz8xAJW"
      },
      "outputs": [],
      "source": [
        "# find text label with highest score\n",
        "labels[pred]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18lOMmwxQD0"
      },
      "source": [
        "Label 2, i.e., “cassette player” is our correctly predicted winner. We can repeat this logic over the entire frgfm/imagenette dataset to get the classification accuracy of CLIP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeOqiAO4xBz2"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "preds = []\n",
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(imagenette), batch_size)):\n",
        "    i_end = min(i + batch_size, len(imagenette))\n",
        "    images = processor(\n",
        "        text=None,\n",
        "        images=imagenette[i:i_end]['image'],\n",
        "        return_tensors='pt'\n",
        "    )['pixel_values'].to(device)\n",
        "    img_emb = model.get_image_features(images)\n",
        "    img_emb = img_emb.detach().cpu().numpy()\n",
        "    scores = np.dot(img_emb, label_emb.T)\n",
        "    preds.extend(np.argmax(scores, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucg7TsUxxW51"
      },
      "outputs": [],
      "source": [
        "true_preds = []\n",
        "for i, label in enumerate(imagenette['label']):\n",
        "    if label == preds[i]:\n",
        "        true_preds.append(1)\n",
        "    else:\n",
        "        true_preds.append(0)\n",
        "\n",
        "sum(true_preds) / len(true_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0NE6qG9xhyv"
      },
      "source": [
        "That gives us an impressive zero-shot accuracy of 98.7%. CLIP proved to be able to accurately predict image classes with little more than some minor reformating of text labels to create sentences.\n",
        "\n",
        "Zero-shot image classification with CLIP is a fascinating use case for high-performance image classification with minimal effort and zero fine-tuning required.\n",
        "\n",
        "Before CLIP, this was not possible. Now that we have CLIP, it is almost too easy. The multi-modality and contrastive pretraining techniques have enabled a technological leap forward.\n",
        "\n",
        "From multi-modal search, zero-shot image classification, and object detection to industry-changing tools like OpenAI’s Dall-E and Stable Diffusion, CLIP has enabled many new use-cases that were previously blocked by insufficient data or compute."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}